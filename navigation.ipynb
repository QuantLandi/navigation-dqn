{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation with Deep-Q Network algorithm\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from agent import Agent\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filename = \"../Banana_Windows_x86_64/Banana.exe\"\n",
    "filename = \"C:/Users/aless/Google Drive/Machine Learning/udacity/deep-rl/navigation_dqn/Banana_Windows_x86_64/Banana.exe\"\n",
    "# filename = \"../Banana_Windows_x86_64/Banana.exe\"\n",
    "\n",
    "env = UnityEnvironment(file_name=filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(agent, env, brain_name, max_n_episodes=2000, max_n_steps=1000,\n",
    "        epsilon_start=1.0, epsilon_min=0.01, epsilon_decay_rate=0.995):\n",
    "    \"\"\"Deep Q-Learning Agent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        max_n_episodes : int\n",
    "            Maximum number of training episodes\n",
    "        max_n_steps : int\n",
    "            Maximum number of steps per episode\n",
    "        epsilon_start : float\n",
    "            Starting value of epsilon, for epsilon-greedy action selection\n",
    "        epsilon_min : float)\n",
    "            Minimum value of epsilon\n",
    "        epsilon_decay_rate : float\n",
    "            Multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    all_scores = []                      # list of scores from all episodes\n",
    "    last_100_scores = deque(maxlen=100)  # last 100 scores\n",
    "    epsilon = epsilon_start              # initialize epsilon\n",
    "    # loop through episodes\n",
    "    is_game_over = False\n",
    "    episode_count = 1\n",
    "    while not is_game_over:\n",
    "        # observe state and initialize score\n",
    "        state = env.reset(train_mode=True)[brain_name].vector_observations[0]\n",
    "        score = 0\n",
    "        # loop through steps within each episode\n",
    "        is_episode_over = False\n",
    "        step_count = 1\n",
    "        while not is_episode_over:\n",
    "            # pick action\n",
    "            action = agent.act(state, epsilon)\n",
    "            # observe updated environment, reward and next state\n",
    "            updated_env = env.step(action)[brain_name]\n",
    "            next_state = updated_env.vector_observations[0]\n",
    "            reward = updated_env.rewards[0]\n",
    "            is_episode_over = updated_env.local_done[0]\n",
    "            # update next state and add reward from step to episode score\n",
    "            agent.step(state, action, reward, next_state, is_episode_over)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            # if episode is over or max_n_steps reached, end loop\n",
    "            # otherwise, do one more step\n",
    "            is_episode_over = is_episode_over or (step_count >= max_n_steps)\n",
    "            step_count += 1\n",
    "        # anneal epsilon\n",
    "        epsilon = max(epsilon_min, epsilon_decay_rate*epsilon)\n",
    "        # keep track of most recent score\n",
    "        last_100_scores.append(score)\n",
    "        all_scores.append(score)\n",
    "        last_100_scores_mean = np.mean(last_100_scores)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode_count, last_100_scores_mean), end=\"\")\n",
    "        completed_100_episodes = episode_count % 100 == 0\n",
    "        if completed_100_episodes:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode_count, last_100_scores_mean))\n",
    "        is_problem_solved = last_100_scores_mean >= 13.0\n",
    "        if is_problem_solved:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'\\\n",
    "                  .format(episode_count, last_100_scores_mean))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'weights.pth')\n",
    "        # if problem solved or max_n_episodes reached, end loop\n",
    "        # otherwise, play one more episode\n",
    "        is_game_over = is_problem_solved or (episode_count >= max_n_episodes)\n",
    "        episode_count += 1\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = \"C:/Users/aless/Google Drive/Machine Learning/udacity/deep-rl/navigation_dqn/Banana_Windows_x86_64/Banana.exe\"\n",
    "env = UnityEnvironment(file_name=filename)\n",
    "# initialize agent\n",
    "agent = Agent(state_size=37, action_size=4, seed=0)\n",
    "brain_name = env.brain_names[0]\n",
    "\n",
    "# track start time\n",
    "start_time = time.time()\n",
    "\n",
    "# train DQN agent\n",
    "scores = dqn(agent, env, brain_name)\n",
    "\n",
    "# track end time and print training time\n",
    "end_time = time.time()\n",
    "training_time = round((end_time - start_time) / 60, 1)\n",
    "print('Total training time: {} minutes'.format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot performance\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "# plot scores for all episodes\n",
    "scores_idx = np.arange(len(scores))\n",
    "plt.plot(scores_idx, scores)\n",
    "# plot moving average of last 100 scores for smoother plotting\n",
    "scores_moving_avg = pd.Series(scores).rolling(100).mean()\n",
    "plt.plot(scores_idx, scores_moving_avg)\n",
    "plt.title('DQN performance over number of episodes', size=14)\n",
    "# plt.title('DQN  Performance Over Number of Episodes')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
